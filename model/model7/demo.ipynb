{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from itertools import combinations\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "from unicodedata import bidirectional\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import models\n",
    "from keras._tf_keras.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras._tf_keras.keras.metrics import MeanAbsoluteError, Accuracy, Precision, Recall, MeanSquaredError\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.optimizers import Adam , RMSprop, Nadam\n",
    "from keras._tf_keras.keras.preprocessing.sequence import pad_sequences \n",
    "from keras._tf_keras.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Masking, InputLayer\n",
    "from keras._tf_keras.keras.regularizers import L1L2, L1, L2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import io\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.load_model(\"modelv1.keras\")\n",
    "class_labels = pd.read_csv(\"class_labels.csv\")[\"0\"].tolist()\n",
    "preprocessor = joblib.load(\"preprocess.pkl\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistics = mp.solutions.holistic\n",
    "holistics = mp_holistics.Holistic(static_image_mode=False, min_detection_confidence=0.65, min_tracking_confidence=0.8)\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None) # show all cols\n",
    "pd.set_option(\"expand_frame_repr\", False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict(landmark_seq, frame_rate, frame_width, frame_height, gesture_action=\"\"):\n",
    "    header = (\n",
    "            [f'{coord}_{i}' for i in range(468) for coord in ('hx', 'hy', 'hz')]+\n",
    "            [f'{coord}_{i}' for i in range(33) for coord in ('px', 'py', 'pz', \"pose_visibility\")]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('lx', 'ly', 'lz')]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('rx', 'ry', 'rz')]+\n",
    "            [\"frame_rate\", \"frame_width\", \"frame_height\", \"frame\"]\n",
    "        )\n",
    "    \n",
    "    data = [\n",
    "        frame_data + [frame_rate, frame_width, frame_height, i] for i, frame_data in enumerate(landmark_seq)\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, columns=df.columns.tolist(), index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    input_df= pd.read_csv(csv_buffer)\n",
    "    categorical_columns = [\"gesture_index\"]\n",
    "    landmark_columns = [f\"{col}\" for col in df.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "    numerical_columns = [\"frame\", \"frame_rate\", \"frame_width\", \"frame_height\"] + [f\"{col}\" for col in df.columns if col.startswith(\"pose_visibility\")]\n",
    "\n",
    "    pd.DataFrame.to_csv(input_df, \"drink2.csv\", index=False)\n",
    "\n",
    "    X_new = np.reshape(input_df, (1, input_df.shape[0], input_df.shape[1]))\n",
    "    prediction = model.predict(X_new)\n",
    "    predicted_labels = [class_labels[np.argmax(pred)] for pred in prediction]\n",
    "    print(predicted_labels)\n",
    "\n",
    "    # # Generate ground truth labels (assuming all samples in input_df are of the same class)\n",
    "    # ground_truth_labels = ['EAT'] * len(predicted_labels)  # Adjust to reflect the actual number of samples and class\n",
    "    \n",
    "    # # Generate confusion matrix\n",
    "    # cm = confusion_matrix(ground_truth_labels, predicted_labels, labels=class_labels)\n",
    "\n",
    "    # # Plot confusion matrix\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    gesture_counts = Counter(predicted_labels)\n",
    "\n",
    "    most_common_gesture = gesture_counts.most_common(1)[0][0]\n",
    "\n",
    "    return most_common_gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image: cv2.typing.MatLike, model):\n",
    "    return image, model.process(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def draw_landmarks(image, model) -> None:\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, model.face_landmarks, mp_holistics.FACEMESH_CONTOURS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=1),\n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.pose_landmarks, mp_holistics.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.GREEN_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.BLUE_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    \n",
    "\n",
    "def extract_keypoints(results):\n",
    "\n",
    "    # Process pose landmarks (if available)\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([face, pose, lh, rh]).tolist()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "['DRINK']\n",
      "Predicted Gesture: DRINK\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "['EAT']\n",
      "Predicted Gesture: EAT\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "isRecording = False\n",
    "landmark_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "    image, results = mediapipe_detection(frame, holistics)\n",
    "    draw_landmarks(image, results)\n",
    "\n",
    "    if isRecording:\n",
    "        holistic_landmarks = extract_keypoints(results)\n",
    "        landmark_seq.append(holistic_landmarks)\n",
    "\n",
    "    cv2.imshow(\"Recording Gestures\", image)\n",
    "    \n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord(\"r\"):\n",
    "            if not isRecording:\n",
    "                isRecording = True \n",
    "                print(\"Recording gesture...\")\n",
    "    elif key == ord(\"s\"):\n",
    "        if isRecording:\n",
    "            isRecording = False \n",
    "\n",
    "    if isRecording and len(landmark_seq) > 30: \n",
    "        pred_gesture = predict(landmark_seq, frame_rate, frame_width, frame_height)\n",
    "        print(f\"Predicted Gesture: {pred_gesture}\")\n",
    "        landmark_seq = []\n",
    "    \n",
    "         \n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# NOTE: model is super finicky - can't do EAT gesture too fast/must hold it in position for a bit to recognize\n",
    "# will try to make this dynamic - no stop and starting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
