{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model(\"data/saved_mode.h5\")\n",
    "class_labels = pd.read_csv(\"data/class_labels.csv\")['gesture'].tolist()\n",
    "\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "\n",
    "landmarks_seq = []\n",
    "recording = False\n",
    "# Function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    # Recolor the frame from BGR to RGB\n",
    "    recolor_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return recolor_frame\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    landmarks = hands.process(frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if landmarks.multi_hand_landmarks:\n",
    "        for hand_landmarks in landmarks.multi_hand_landmarks:\n",
    "            if recording:\n",
    "                # Extract landmarks\n",
    "                landmarks = [lm for lm in hand_landmarks.landmark]\n",
    "                landmarks_flat = [coord for lm in landmarks for coord in (lm.x, lm.y, lm.z)]\n",
    "                landmarks_seq.append(landmarks_flat)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "# Function to extract features from a frame (example using landmarks)\n",
    "def extract_features(frame):\n",
    "    # Implement your feature extraction logic here\n",
    "    # Example: extract landmarks from the frame\n",
    "    landmarks = extract_landmarks(frame)\n",
    "\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "# Function to predict gesture from a preprocessed frame\n",
    "def predict_gesture(preprocessed_frame):\n",
    "    # Reshape and scale features as per your model's expectations\n",
    "    features = scaler.transform(preprocessed_frame)  # Use the same scaler as in training\n",
    "    \n",
    "    # Reshape features for LSTM input (1 sample, 1 timestep, features)\n",
    "    X = features.reshape((1, 1, len(features)))\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X)\n",
    "    predicted_label = class_labels[np.argmax(prediction)]\n",
    "    return predicted_label\n",
    "\n",
    "# Initialize video capture from webcam\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a video file path if working with a video file\n",
    "\n",
    "# Define frame dimensions\n",
    "width, height = cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # Example dimensions, adjust according to your model's input size\n",
    "\n",
    "# Load scaler and any other preprocessing steps used during training\n",
    "scaler = StandardScaler()  # Example, replace with your actual preprocessing steps\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Preprocess the frame\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "    \n",
    "    # Extract features from the preprocessed frame\n",
    "    extracted_features = extract_features(processed_frame)\n",
    "    \n",
    "    # # Perform preprocessing steps used during training (e.g., scaling)\n",
    "    # scaled_features = scaler.fit_transform(extracted_features)\n",
    "    \n",
    "    # # Make prediction\n",
    "    # predicted_gesture = predict_gesture(scaled_features)\n",
    "    \n",
    "    # # Display prediction text on the frame\n",
    "    # cv2.putText(frame, predicted_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Real-time Gesture Recognition', frame)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
