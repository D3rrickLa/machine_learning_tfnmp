{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from itertools import combinations\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "from unicodedata import bidirectional\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import models\n",
    "from keras._tf_keras.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras._tf_keras.keras.metrics import MeanAbsoluteError, Accuracy, Precision, Recall, MeanSquaredError\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.optimizers import Adam , RMSprop, Nadam\n",
    "from keras._tf_keras.keras.preprocessing.sequence import pad_sequences \n",
    "from keras._tf_keras.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Masking, InputLayer\n",
    "from keras._tf_keras.keras.regularizers import L1L2, L1, L2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import io\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import joblib\n",
    "\n",
    "from engineering import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(r\"keras_models\\model_dev_11722277649426481100.keras\")\n",
    "class_labels = pd.read_csv(\"class_labels.csv\")[\"gesture\"].tolist()\n",
    "preprocessor = joblib.load(\"preprocessor.pkl\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistics = mp.solutions.holistic\n",
    "holistics = mp_holistics.Holistic(static_image_mode=False, min_detection_confidence=0.65, min_tracking_confidence=0.8)\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None) # show all cols\n",
    "pd.set_option(\"expand_frame_repr\", False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hand_motion_features(df: pd.DataFrame, landmark_cols: list) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    _ = fe.calculate_temporal_features(df_copy, landmark_cols)\n",
    "    df_combined = df_copy.loc[:, ~df_copy.columns.duplicated()] # removed any duplicate columns\n",
    "    return df_combined\n",
    "\n",
    "def predict(landmark_seq, frame_rate, frame_width, frame_height, gesture_action=\"\"):\n",
    "    header = (\n",
    "            [f'{coord}_{i}' for i in range(468) for coord in ('hx', 'hy', 'hz')]+\n",
    "            [f'{coord}_{i}' for i in range(33) for coord in ('px', 'py', 'pz', \"pose_visibility\")]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('lx', 'ly', 'lz')]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('rx', 'ry', 'rz')]+\n",
    "            [\"frame_rate\", \"frame_width\", \"frame_height\", \"frame\", \"gesture_index\"]\n",
    "        )\n",
    "    \n",
    "    data = [\n",
    "        frame_data + [frame_rate, frame_width, frame_height, i, time.time_ns()]  for i, frame_data in enumerate(landmark_seq)\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, columns=df.columns.tolist(), index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    input_df= pd.read_csv(csv_buffer)\n",
    "    landmark_columns = [f\"{col}\" for col in input_df.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "    categorical_columns = [\"gesture_index\"]\n",
    "    numerical_columns = [\"frame\", \"frame_rate\", \"frame_width\", \"frame_height\"] + [f\"{col}\" for col in input_df.columns if col.startswith(\"pose_visibility\")]\n",
    "    derived_features =  [f\"{feat}_{col}\" for feat in [\"velocity\", \"acceleration\", \"jerk\"] for col in landmark_columns if col.startswith((\"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "    time_series_columns = landmark_columns + derived_features     \n",
    "    res = [item for item in landmark_columns if item.startswith((\"r\", \"l\"))]\n",
    "\n",
    "    # pd.DataFrame.to_csv(input_df, \"drink2.csv\", index=False)\n",
    "    X_new_fe = calculate_hand_motion_features(input_df, res)\n",
    "    X_new_pre = preprocessor.transform(X_new_fe)\n",
    "    X_new_pre.drop(columns=\"remainder__gesture_index\", inplace=True) # gesture_index isn't the name after transforming\n",
    "    X_new = np.reshape(X_new_pre, (1, X_new_pre.shape[0], X_new_pre.shape[1])) # 1 because only 1 recording coming in\n",
    "    prediction = model.predict(X_new, verbose=0)\n",
    "    predicted_labels = [class_labels[np.argmax(pred)] for pred in prediction]\n",
    "    \n",
    "    print(prediction)\n",
    "\n",
    "    # Generate ground truth labels (assuming all samples in input_df are of the same class)\n",
    "    # ground_truth_labels = ['EAT'] * len(predicted_labels)  # Adjust to reflect the actual number of samples and class\n",
    "    \n",
    "    # # Generate confusion matrix\n",
    "    # cm = confusion_matrix(ground_truth_labels, predicted_labels, labels=class_labels)\n",
    "\n",
    "    # # # Plot confusion matrix\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    gesture_counts = Counter(predicted_labels)\n",
    "\n",
    "    most_common_gesture = gesture_counts.most_common(1)[0][0]\n",
    "\n",
    "    return most_common_gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image: cv2.typing.MatLike, model):\n",
    "    return image, model.process(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def draw_landmarks(image, model) -> None:\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, model.face_landmarks, mp_holistics.FACEMESH_CONTOURS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=1),\n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.pose_landmarks, mp_holistics.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.GREEN_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.BLUE_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    \n",
    "\n",
    "def extract_keypoints(results):\n",
    "\n",
    "    # Process pose landmarks (if available)\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([face, pose, lh, rh]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n",
      "[[0.4943265  0.00228037 0.01359412 0.47378796 0.01601113]]\n",
      "Predicted Gesture: ALL-DONE\n",
      "[[8.7187213e-01 2.9566049e-04 1.3060041e-02 1.1365127e-01 1.1208643e-03]]\n",
      "Predicted Gesture: ALL-DONE\n",
      "[[7.5430970e-04 9.9485889e-02 8.9691353e-01 1.0425136e-03 1.8038074e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[6.6631805e-04 1.3293502e-01 8.6241829e-01 1.2940809e-03 2.6863182e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[6.3436379e-04 1.4805101e-01 8.4627026e-01 1.5506952e-03 3.4936490e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[7.2963920e-04 1.3868175e-01 8.5639352e-01 1.3277807e-03 2.8672635e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[6.2582153e-04 1.4355332e-01 8.4976363e-01 2.4372039e-03 3.6200103e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[0.00173752 0.05501077 0.9293851  0.00853249 0.00533405]]\n",
      "Predicted Gesture: EAT\n",
      "[[9.1088939e-01 7.1045035e-04 1.4423810e-02 7.2608553e-02 1.3678380e-03]]\n",
      "Predicted Gesture: ALL-DONE\n",
      "[[0.9330214  0.00611544 0.03000146 0.02606928 0.00479246]]\n",
      "Predicted Gesture: ALL-DONE\n",
      "[[7.3314761e-04 7.3287360e-02 9.2417520e-01 7.5486070e-04 1.0493655e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[7.2698260e-04 7.8129336e-02 9.1904569e-01 8.0832944e-04 1.2896475e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[6.1431830e-04 7.6336749e-02 9.2078173e-01 7.7037042e-04 1.4968476e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[6.1395217e-04 7.5622886e-02 9.2142552e-01 7.2324096e-04 1.6144329e-03]]\n",
      "Predicted Gesture: EAT\n",
      "[[5.6182005e-04 9.8224923e-02 8.9727789e-01 1.3634817e-03 2.5718589e-03]]\n",
      "Predicted Gesture: EAT\n",
      "Recording gesture...\n",
      "[[0.01410395 0.00213424 0.00198036 0.8544159  0.12736557]]\n",
      "Predicted Gesture: MORE\n",
      "[[0.41327688 0.05466556 0.2287017  0.22797866 0.07537717]]\n",
      "Predicted Gesture: ALL-DONE\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "isRecording = False\n",
    "landmark_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "    image, results = mediapipe_detection(frame, holistics)\n",
    "    draw_landmarks(image, results)\n",
    "\n",
    "    if isRecording:\n",
    "        holistic_landmarks = extract_keypoints(results)\n",
    "        landmark_seq.append(holistic_landmarks)\n",
    "\n",
    "    cv2.imshow(\"Recording Gestures\", image)\n",
    "    \n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord(\"r\"):\n",
    "            if not isRecording:\n",
    "                isRecording = True \n",
    "                print(\"Recording gesture...\")\n",
    "    elif key == ord(\"s\"):\n",
    "        if isRecording:\n",
    "            isRecording = False \n",
    "\n",
    "    if isRecording and len(landmark_seq) == 30: \n",
    "        pred_gesture = predict(landmark_seq, frame_rate, frame_width, frame_height)\n",
    "        print(f\"Predicted Gesture: {pred_gesture}\")\n",
    "        landmark_seq = []\n",
    "    \n",
    "         \n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
