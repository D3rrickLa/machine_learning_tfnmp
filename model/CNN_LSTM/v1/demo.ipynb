{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from itertools import combinations\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "from unicodedata import bidirectional\n",
    "from enum import Enum\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import models\n",
    "from keras._tf_keras.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras._tf_keras.keras.metrics import MeanAbsoluteError, Accuracy, Precision, Recall, MeanSquaredError\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.optimizers import Adam , RMSprop, Nadam\n",
    "from keras._tf_keras.keras.preprocessing.sequence import pad_sequences \n",
    "from keras._tf_keras.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Masking, InputLayer\n",
    "from keras._tf_keras.keras.regularizers import L1L2, L1, L2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import io\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import joblib\n",
    "\n",
    "from engineering import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(r\"keras_models\\model_dev_11722293967753330200.keras\")\n",
    "class_labels = pd.read_csv(\"class_labels.csv\")[\"gesture\"].tolist()\n",
    "preprocessor = joblib.load(\"preprocessor.pkl\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistics = mp.solutions.holistic\n",
    "holistics = mp_holistics.Holistic(static_image_mode=False, min_detection_confidence=0.65, min_tracking_confidence=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgramShortcuts(Enum):\n",
    "    quit = ord(u\"q\")\n",
    "    start = ord(u\"r\")\n",
    "    stop = ord(u\"s\")\n",
    "\n",
    "def calculate_hand_motion_features(df: pd.DataFrame, landmark_cols: list) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    _ = fe.calculate_temporal_features(df_copy, landmark_cols)\n",
    "    df_combined = df_copy.loc[:, ~df_copy.columns.duplicated()] # removed any duplicate columns\n",
    "    return df_combined\n",
    "\n",
    "def predict(landmark_seq, frame_rate, frame_width, frame_height, gesture_action=\"\"):\n",
    "    header = (\n",
    "            [f'{coord}_{i}' for i in range(468) for coord in ('hx', 'hy', 'hz')]+\n",
    "            [f'{coord}_{i}' for i in range(33) for coord in ('px', 'py', 'pz', \"pose_visibility\")]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('lx', 'ly', 'lz')]+\n",
    "            [f'{coord}_{i}' for i in range(21) for coord in ('rx', 'ry', 'rz')]+\n",
    "            [\"frame_rate\", \"frame_width\", \"frame_height\", \"frame\", \"gesture_index\"]\n",
    "        )\n",
    "    \n",
    "    data = [\n",
    "        frame_data + [frame_rate, frame_width, frame_height, i, time.time_ns()]  for i, frame_data in enumerate(landmark_seq)\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, columns=df.columns.tolist(), index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    input_df= pd.read_csv(csv_buffer)\n",
    "    landmark_columns = [f\"{col}\" for col in input_df.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "    res = [item for item in landmark_columns if item.startswith((\"r\", \"l\"))]\n",
    "\n",
    "    # pd.DataFrame.to_csv(input_df, \"drink2.csv\", index=False)\n",
    "    X_new_fe = calculate_hand_motion_features(input_df, res)\n",
    "    X_new_pre = preprocessor.transform(X_new_fe)\n",
    "    X_new_pre.drop(columns=\"remainder__gesture_index\", inplace=True) # gesture_index isn't the name after transforming\n",
    "    X_new = np.reshape(X_new_pre, (1, X_new_pre.shape[0], X_new_pre.shape[1])) # 1 because only 1 recording coming in\n",
    "    prediction = model.predict(X_new, verbose=0)\n",
    "    predicted_labels = [class_labels[np.argmax(pred)] for pred in prediction]\n",
    "    \n",
    "    # Generate ground truth labels (assuming all samples in input_df are of the same class)\n",
    "    # ground_truth_labels = ['EAT'] * len(predicted_labels)  # Adjust to reflect the actual number of samples and class\n",
    "    \n",
    "    # # Generate confusion matrix\n",
    "    # cm = confusion_matrix(ground_truth_labels, predicted_labels, labels=class_labels)\n",
    "\n",
    "    # # # Plot confusion matrix\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    gesture_counts = Counter(predicted_labels)\n",
    "\n",
    "    most_common_gesture = gesture_counts.most_common(1)[0][0]\n",
    "\n",
    "    return most_common_gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image: cv2.typing.MatLike, model):\n",
    "    return image, model.process(cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def draw_landmarks(image, model) -> None:\n",
    "    # mp_drawing.draw_landmarks(image, model.face_landmarks, mp_holistics.FACEMESH_CONTOURS,\n",
    "    #                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=1),\n",
    "    #                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "    #                         )\n",
    "    mp_drawing.draw_landmarks(image, model.pose_landmarks, mp_holistics.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS, \n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.GREEN_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    mp_drawing.draw_landmarks(image, model.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=mp_drawing.BLUE_COLOR, thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(181, 135, 230), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "    \n",
    "\n",
    "def extract_keypoints(results):\n",
    "\n",
    "    # Process pose landmarks (if available)\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([face, pose, lh, rh]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n",
      "Predicted Gesture: EAT\n",
      "Recording stopped...\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "isRecording = False\n",
    "landmark_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "    image, results = mediapipe_detection(frame, holistics)\n",
    "    draw_landmarks(image, results)\n",
    "\n",
    "    if isRecording:\n",
    "        holistic_landmarks = extract_keypoints(results)\n",
    "        landmark_seq.append(holistic_landmarks)\n",
    "\n",
    "    cv2.imshow(\"Recording Gestures\", image)\n",
    "    \n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    match key:\n",
    "        case ProgramShortcuts.start.value:\n",
    "            if not isRecording:\n",
    "                isRecording = True \n",
    "                print(\"Recording gesture...\")\n",
    "        case ProgramShortcuts.stop.value:\n",
    "            if isRecording:\n",
    "                isRecording = False \n",
    "                print(\"Recording stopped...\")\n",
    "        case ProgramShortcuts.quit.value:\n",
    "            break\n",
    "\n",
    "    if isRecording and len(landmark_seq) == 30: \n",
    "        pred_gesture = predict(landmark_seq, frame_rate, frame_width, frame_height)\n",
    "        print(f\"Predicted Gesture: {pred_gesture}\")\n",
    "        landmark_seq = []    \n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
