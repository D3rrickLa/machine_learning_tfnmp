{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mediapipe as mp\n",
    "import io\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gestures...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "Predicted Gesture: WAVING\n",
      "Recording gestures...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Predicted Gesture: WAVING\n",
      "Recording gestures...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Predicted Gesture: WAVING\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Predicted Gesture: WAVING\n",
      "Recording gestures...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Predicted Gesture: MORE\n",
      "Recording gestures...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Predicted Gesture: MORE\n",
      "Recording gestures...\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Predicted Gesture: MORE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = models.load_model(\"data/lstm_v3.h5\")\n",
    "class_labels = pd.read_csv(\"data/class_labels.csv\")['gesture'].tolist()\n",
    "scaler = joblib.load('data/scaler.pkl')\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# feature engineering \n",
    "def calculate_hand_motion_features(df, landmark_cols):\n",
    "    new_cols = {}\n",
    "\n",
    "    for col in landmark_cols:\n",
    "        new_cols[f\"velocity_{col}\"] = df[col].diff().fillna(0)\n",
    "        new_cols[f\"acceleration_{col}\"] = new_cols[f\"velocity_{col}\"].diff().fillna(0)\n",
    "        \n",
    "    # Calculate pairwise distances between all landmarks\n",
    "    landmark_pairs = list(combinations(landmark_cols, 2))\n",
    "    for (col1, col2) in landmark_pairs:\n",
    "        idx1 = col1[1:]  # Get index part from 'x0', 'y0', etc.\n",
    "        idx2 = col2[1:]\n",
    "        if idx1 == idx2:\n",
    "            continue\n",
    "        x1, y1, z1 = f'x{idx1}', f'y{idx1}', f'z{idx1}'\n",
    "        x2, y2, z2 = f'x{idx2}', f'y{idx2}', f'z{idx2}'\n",
    "        distance_col = f'distance_{idx1}_{idx2}'\n",
    "        new_cols[distance_col] = np.sqrt((df[x1] - df[x2])**2 + (df[y1] - df[y2])**2 + (df[z1] - df[z2])**2)\n",
    "    \n",
    "    new_df = pd.DataFrame(new_cols)\n",
    "\n",
    "    return pd.concat([df, new_df], axis=1)\n",
    "\n",
    "def create_sequences(data, timesteps):\n",
    "    sequences = [] \n",
    "    for i in range(len(data) - timesteps + 1):\n",
    "        sequences.append(data[i:i + timesteps])\n",
    "\n",
    "    return np.array(sequences)\n",
    "\n",
    "def predict_gesture(landmarks_seq, frame_rate, frame_width, frame_height, gesture_action=\"\"):\n",
    "    gesture_index = int(time.time())\n",
    "\n",
    "\n",
    "    header = ['frame'] + [f'{coord}_{i}' for i in range(21) for coord in ('x', 'y', 'z')] + ['frame_rate', 'frame_width', 'frame_height', 'gesture', 'gesture_index']\n",
    "    data = [[i] + frame_data + [frame_rate, frame_width, frame_height, gesture_action, gesture_index] for i, frame_data in enumerate(landmarks_seq)]\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    \n",
    "    landmark_cols = [col for col in df.columns if col.startswith((\"x\", \"y\", \"z\"))]\n",
    "    dataframe = calculate_hand_motion_features(df.copy(), landmark_cols)\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, columns=dataframe.columns.tolist(), index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    input_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "    features = [col for col in input_df.columns if col not in [\"gesture\"]]\n",
    "    \n",
    "    input_df[features] = scaler.transform(input_df[features])\n",
    "\n",
    "    X_new = create_sequences(input_df[features].values, timesteps=10)\n",
    "\n",
    "    prediction = model.predict(X_new)\n",
    "    predicted_labels = [class_labels[np.argmax(pred)] for pred in prediction]\n",
    "\n",
    "    gesture_counts = Counter(predicted_labels)\n",
    "\n",
    "    most_common_gesture = gesture_counts.most_common(1)[0][0]\n",
    "\n",
    "    return most_common_gesture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def record():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    recording = False\n",
    "    landmarks_seq = []\n",
    "\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                if recording:\n",
    "                    # Extract landmarks\n",
    "                    landmarks = [lm for lm in hand_landmarks.landmark]\n",
    "                    landmarks_flat = [coord for lm in landmarks for coord in (lm.x, lm.y, lm.z)]\n",
    "                    landmarks_seq.append(landmarks_flat)\n",
    "\n",
    "        cv2.imshow(\"TEST\", frame)\n",
    "\n",
    "        key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        # Start recording on 'r' key press\n",
    "        if key == ord('r'):\n",
    "            recording = True\n",
    "            print(\"Recording gestures...\")\n",
    "        \n",
    "        # Stop recording and predict on 's' key press\n",
    "        elif key == ord('s'):\n",
    "            recording = False\n",
    "            if landmarks_seq:\n",
    "                # Predict gesture from recorded landmarks sequence\n",
    "                predicted_gesture = predict_gesture(landmarks_seq, frame_rate, frame_width, frame_height)\n",
    "                print(f\"Predicted Gesture: {predicted_gesture}\")\n",
    "            else:\n",
    "                print(\"No gestures recorded.\")\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "record()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
