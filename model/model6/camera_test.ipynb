{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mediapipe as mp\n",
    "import io\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.discriminant_analysis import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gestures...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 405 and 404 for '{{node sequential_1/lstm_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/lstm_1/strided_slice_1, sequential_1/lstm_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [1,405], [404,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(1, 405), dtype=float32)\n  • states=('tf.Tensor(shape=(1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 64), dtype=float32)')\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    118\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m--> 120\u001b[0m \u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mrecord\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m recording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m landmarks_seq:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# Predict gesture from recorded landmarks sequence\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     predicted_gesture \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandmarks_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Gesture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_gesture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m, in \u001b[0;36mpredict_gesture\u001b[1;34m(landmarks_seq, frame_rate, frame_width, frame_height, gesture_action)\u001b[0m\n\u001b[0;32m     52\u001b[0m input_df[features] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(input_df[features])\n\u001b[0;32m     54\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_df[features]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, input_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(features)))\n\u001b[1;32m---> 56\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [class_labels[np\u001b[38;5;241m.\u001b[39margmax(pred)] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m prediction]\n\u001b[0;32m     59\u001b[0m gesture_counts \u001b[38;5;241m=\u001b[39m Counter(predicted_labels)\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 405 and 404 for '{{node sequential_1/lstm_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/lstm_1/strided_slice_1, sequential_1/lstm_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [1,405], [404,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(1, 405), dtype=float32)\n  • states=('tf.Tensor(shape=(1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 64), dtype=float32)')\n  • training=False"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = models.load_model(\"data/lstm_v3.keras\")\n",
    "class_labels = pd.read_csv(\"data/class_labels.csv\")['gesture'].tolist()\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# feature engineering \n",
    "def calculate_hand_motion_features(df, landmark_cols):\n",
    "    new_cols = {}\n",
    "\n",
    "    for col in landmark_cols:\n",
    "        new_cols[f\"velocity_{col}\"] = df[col].diff().fillna(0)\n",
    "        new_cols[f\"acceleration_{col}\"] = new_cols[f\"velocity_{col}\"].diff().fillna(0)\n",
    "        \n",
    "    # Calculate pairwise distances between all landmarks\n",
    "    landmark_pairs = list(combinations(landmark_cols, 2))\n",
    "    for (col1, col2) in landmark_pairs:\n",
    "        idx1 = col1[1:]  # Get index part from 'x0', 'y0', etc.\n",
    "        idx2 = col2[1:]\n",
    "        if idx1 == idx2:\n",
    "            continue\n",
    "        x1, y1, z1 = f'x{idx1}', f'y{idx1}', f'z{idx1}'\n",
    "        x2, y2, z2 = f'x{idx2}', f'y{idx2}', f'z{idx2}'\n",
    "        distance_col = f'distance_{idx1}_{idx2}'\n",
    "        new_cols[distance_col] = np.sqrt((df[x1] - df[x2])**2 + (df[y1] - df[y2])**2 + (df[z1] - df[z2])**2)\n",
    "    \n",
    "    new_df = pd.DataFrame(new_cols)\n",
    "\n",
    "    return pd.concat([df, new_df], axis=1)\n",
    "\n",
    "def predict_gesture(landmarks_seq, frame_rate, frame_width, frame_height, gesture_action=\"\"):\n",
    "    gesture_index = int(time.time())\n",
    "\n",
    "    header = ['frame'] + [f'{coord}_{i}' for i in range(21) for coord in ('x', 'y', 'z')] + ['frame_rate', 'frame_width', 'frame_height', 'gesture', 'gesture_index', 'distance_0_1']\n",
    "    data = [[i] + frame_data + [frame_rate, frame_width, frame_height, gesture_action, gesture_index, 0] for i, frame_data in enumerate(landmarks_seq)]  # Initialize distance_0_1 as 0\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    landmark_cols = [col for col in df.columns if col.startswith((\"x\", \"y\", \"z\"))]\n",
    "\n",
    "    dataframe = calculate_hand_motion_features(df.copy(), landmark_cols)\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, columns = [*dataframe])\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    input_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "    features = [col for col in input_df.columns if col not in [\"frame\", \"gesture\"]]\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    input_df[features] = scaler.fit_transform(input_df[features])\n",
    "\n",
    "    X_new = input_df[features].values.reshape((1, input_df.shape[0], len(features)))\n",
    "\n",
    "    prediction = model.predict(X_new)\n",
    "    predicted_labels = [class_labels[np.argmax(pred)] for pred in prediction]\n",
    "\n",
    "    gesture_counts = Counter(predicted_labels)\n",
    "\n",
    "    most_common_gesture = gesture_counts.most_common(1)[0][0]\n",
    "\n",
    "    return most_common_gesture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def record():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    recording = False\n",
    "    landmarks_seq = []\n",
    "\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                if recording:\n",
    "                    # Extract landmarks\n",
    "                    landmarks = [lm for lm in hand_landmarks.landmark]\n",
    "                    landmarks_flat = [coord for lm in landmarks for coord in (lm.x, lm.y, lm.z)]\n",
    "                    landmarks_seq.append(landmarks_flat)\n",
    "\n",
    "        cv2.imshow(\"TEST\", frame)\n",
    "\n",
    "        key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        # Start recording on 'r' key press\n",
    "        if key == ord('r'):\n",
    "            recording = True\n",
    "            print(\"Recording gestures...\")\n",
    "        \n",
    "        # Stop recording and predict on 's' key press\n",
    "        elif key == ord('s'):\n",
    "            recording = False\n",
    "            if landmarks_seq:\n",
    "                # Predict gesture from recorded landmarks sequence\n",
    "                predicted_gesture = predict_gesture(landmarks_seq, frame_rate, frame_width, frame_height)\n",
    "                print(f\"Predicted Gesture: {predicted_gesture}\")\n",
    "            else:\n",
    "                print(\"No gestures recorded.\")\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "record()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
