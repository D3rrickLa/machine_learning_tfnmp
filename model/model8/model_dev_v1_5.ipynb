{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from FeatureEngineering import FeatureEngineering as fe\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code, we aren't splitting by recording frames per gesture, but rather by the gestures as a whole so if WAVE has 10 gesture recordings, 8 go to train and 2 to test\n",
    "\n",
    "\n",
    "\n",
    "def create_dataframe_from_data(input_path: str):\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "            data = np.load(file_path, allow_pickle=True)\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            gesture = file_name.split(\"_\")[0]\n",
    "            gesture_index = int(file_name.split(\"_\")[1].split(\".\")[0])\n",
    "\n",
    "            # add two new columns to df \n",
    "            df[\"gesture\"] = gesture\n",
    "            df[\"gesture_index\"] = gesture_index\n",
    "\n",
    "            dataframes.append(df)\n",
    "            df.sort_values(by=\"frame\", inplace=True)\n",
    "    \n",
    "    return pd.concat(dataframes, ignore_index=True) if len(dataframes) > 0 else ValueError(\"Dataframe is empty\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_path = \"C:\\\\Users\\\\Gen3r\\\\Documents\\\\capstone\\\\ml_model\\\\data\\\\data_3\"\n",
    "\n",
    "dataframe = create_dataframe_from_data(input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_dict(df: pd.DataFrame):\n",
    "\n",
    "    diction = {x: [] for x in np.unique(df[\"gesture\"].values.tolist())}\n",
    "    grouped = df.groupby(\"gesture_index\")\n",
    "    for gesture_index, gesture_data in grouped:\n",
    "        gesture = np.unique(gesture_data[\"gesture\"].values.tolist())[0]\n",
    "        temp = diction[gesture]  + [gesture_index]\n",
    "        diction.update({gesture:temp})\n",
    "\n",
    "    return diction\n",
    "\n",
    "def split_dataset(df: pd.DataFrame, target_label: str, additional_targets: list=None, train_ratio=0.8, test_ratio=0.2):\n",
    "    \n",
    "    assert train_ratio + test_ratio == 1.0, \"ratios must sum to 1.\"\n",
    "\n",
    "    gesture_index_dict = create_index_dict(dataframe)\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Calculate number of indices for training set and split\n",
    "    for gesture, indices in gesture_index_dict.items():\n",
    "        n_train = math.ceil(len(indices) * train_ratio)\n",
    "        train_indices.extend(indices[:n_train])\n",
    "        test_indices.extend(indices[n_train:])\n",
    "\n",
    "    grouped = dataframe.groupby(\"gesture_index\")\n",
    "\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    for idx in train_indices:\n",
    "        train_frames.append(grouped.get_group(idx))\n",
    "        \n",
    "    for idx in test_indices:\n",
    "        test_frames.append(grouped.get_group(idx))\n",
    "    \n",
    "    # Concatenate the dataframes to create the final train and test sets\n",
    "    train_set = pd.concat(train_frames).reset_index(drop=True)\n",
    "    test_set = pd.concat(test_frames).reset_index(drop=True)\n",
    "\n",
    "    # Separate X and y\n",
    "    X_train = train_set.drop(columns=[target_label])\n",
    "    y_train = train_set[[target_label] + additional_targets] if additional_targets else train_set[[target_label]]\n",
    "    X_test = test_set.drop(columns=[target_label])\n",
    "    y_test = test_set[[target_label] + additional_targets] if additional_targets else test_set[[target_label]]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_dataset(dataframe, \"gesture\", [\"frame\", \"gesture_index\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(16740, 1667) (8370, 1667) (8370, 1667)\n",
      "(4140, 1667) (2070, 1667) (2070, 1667)\n",
      "(8370, 3) (2070, 3)\n",
      "hx_0             float32\n",
      "hy_0             float32\n",
      "hz_0             float32\n",
      "hx_1             float32\n",
      "hy_1             float32\n",
      "                  ...   \n",
      "frame_rate       float32\n",
      "frame_width      float32\n",
      "frame_height     float32\n",
      "frame            float32\n",
      "gesture_index      int64\n",
      "Length: 1667, dtype: object\n",
      "hx_0             float64\n",
      "hy_0             float64\n",
      "hz_0             float64\n",
      "hx_1             float64\n",
      "hy_1             float64\n",
      "                  ...   \n",
      "frame_rate       float32\n",
      "frame_width      float32\n",
      "frame_height     float32\n",
      "frame            float32\n",
      "gesture_index      int64\n",
      "Length: 1667, dtype: object\n",
      "0\n",
      "0\n",
      "0\n",
      "(16740, 1667)\n",
      "0.078125\n",
      "109.125\n",
      "(4140, 1667)\n",
      "0.046875\n",
      "29.015625\n"
     ]
    }
   ],
   "source": [
    "def augment_model(df: pd.DataFrame, noise_level=0.0, translation_vector=None, rotation_angle=0.0):\n",
    "    df_augmented = df.copy()\n",
    "\n",
    "\n",
    "    landmark_columns = [f\"{col}\" for col in df_augmented.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "    \n",
    "    num_body_parts = (\"h\", \"p\", \"l\", \"r\")\n",
    "\n",
    "    x_columns = [col for col in landmark_columns if any(col.startswith(f'{i}x') for i in num_body_parts)] # this way works because of how i is defined before hand... don't really know\n",
    "    y_columns = [col for col in landmark_columns if any(col.startswith(f'{i}y') for i in num_body_parts)]\n",
    "    z_columns = [col for col in landmark_columns if any(col.startswith(f'{i}z') for i in num_body_parts)]\n",
    "\n",
    "\n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level, df[x_columns + y_columns + z_columns].shape)\n",
    "        df_augmented[x_columns + y_columns + z_columns] += noise\n",
    "\n",
    "    # Apply translation\n",
    "    if translation_vector is not None:\n",
    "        for i, col in enumerate(x_columns):\n",
    "            df_augmented[col] += translation_vector[i % 3]\n",
    "        for i, col in enumerate(y_columns):\n",
    "            df_augmented[col] += translation_vector[i % 3]\n",
    "        for i, col in enumerate(z_columns):\n",
    "            df_augmented[col] += translation_vector[i % 3]\n",
    "\n",
    "    # Apply rotation around the Z-axis\n",
    "    if rotation_angle != 0:\n",
    "        angle_radians = np.radians(rotation_angle)\n",
    "        cos_angle = np.cos(angle_radians)\n",
    "        sin_angle = np.sin(angle_radians)\n",
    "\n",
    "        for col in x_columns:\n",
    "            y_col = col.replace('x', 'y')\n",
    "            df_augmented[col], df_augmented[y_col] = (cos_angle * df_augmented[col] - sin_angle * df_augmented[y_col],\n",
    "                                                      sin_angle * df_augmented[col] + cos_angle * df_augmented[y_col])\n",
    "    \n",
    "    # making the gesture index of the augment different to be added into the dataset back\n",
    "    # will need to double up on the y train and test as well\n",
    "\n",
    "    if \"gesture_index\" in df_augmented.columns:\n",
    "        cur_time = time.time_ns()\n",
    "        df_augmented[\"gesture_index\"] += cur_time\n",
    "\n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "def calculate_hand_motion_feature(df: pd.DataFrame, landmark_cols: list):\n",
    "    df_copy = df.copy()\n",
    "    print(df_copy.shape)\n",
    "    s = time.process_time()\n",
    "    df_elapsed = fe.calculate_elapsed_time(df_copy)    \n",
    "    print(time.process_time()-s)\n",
    "\n",
    "    s = time.process_time()\n",
    "    df_temporal = fe.calculate_temporal_features(df_copy, landmark_cols)\n",
    "    print(time.process_time()-s)\n",
    "    # df_stats = fe.calculate_temporal_stats(df_copy, landmark_cols)\n",
    "    # df_pairwise = fe.calculate_landmark_distances(df_copy, landmark_cols)\n",
    "    # df_angle = fe.calculate_landmark_angles(df_copy, landmark_cols)\n",
    "    # df_combined = pd.concat([df_copy, df_angle], axis=1)\n",
    "    \n",
    "    # Ensure there are no duplicate columns\n",
    "    df_combined = df_copy.loc[:,~df_copy.columns.duplicated()]\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "X_train_augmented = augment_model(X_train, noise_level=0.05, translation_vector=[0.6, -0.5, 0.05], rotation_angle=45)\n",
    "X_test_augmented = augment_model(X_test, noise_level=0.05, translation_vector=[0.6, -0.5, 0.05], rotation_angle=45)\n",
    "\n",
    "print(X_test.equals(X_test_augmented))\n",
    "\n",
    "\n",
    "X_train_combined = pd.concat([X_train, X_train_augmented], axis=0, ignore_index=True)\n",
    "X_test_combined = pd.concat([X_test, X_test_augmented], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "# print(X_train_combined.shape, X_train_augmented.shape, X_train.shape)\n",
    "# print(X_test_combined.shape, X_test_augmented.shape, X_test.shape)\n",
    "# print(y_train.shape, y_test.shape)\n",
    "\n",
    "# print(X_train.dtypes)\n",
    "# print(X_train_augmented.dtypes)\n",
    "# print(X_train_combined.index.duplicated().sum())  # Should be 0\n",
    "# print(X_train_combined.isnull().sum().sum())  # Should be 0\n",
    "# print(np.isinf(X_train_combined).sum().sum())  # Should be 0\n",
    "\n",
    "landmark_columns = [f\"{col}\" for col in dataframe.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "categorical_columns = [\"gesture_index\"]\n",
    "numerical_columns = [\"frame\", \"frame_rate\", \"frame_width\", \"frame_height\"] + [f\"{col}\" for col in dataframe.columns if col.startswith(\"pose_visibility\")]\n",
    "derived_features =  ['elapsed_time'] + \\\n",
    "                    [f\"{feat}_{col}\" for feat in [\"velocity\", \"acceleration\", \"jerk\"] for col in landmark_columns if col.startswith((\"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "time_series_columns = landmark_columns + derived_features     \n",
    "\n",
    "\n",
    "\n",
    "res = [item for item in landmark_columns if item.startswith((\"r\", \"l\"))]\n",
    "X_train_fe = calculate_hand_motion_feature(X_train_combined, res)\n",
    "X_test_fe = calculate_hand_motion_feature(X_test_combined, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16740, 3)\n",
      "(4140, 3)\n",
      "(279,) (69,)\n"
     ]
    }
   ],
   "source": [
    "# code to reshape y, to have both the new augmented data and resgaoe shape the labesl\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from keras._tf_keras.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def reshape_y_labels(df: pd.DataFrame):\n",
    "    \n",
    "    df = pd.concat([df, df])\n",
    "    print(df.shape)\n",
    "    unique_sequences = []\n",
    "    for _, group in df.groupby(\"gesture_index\"):\n",
    "        reset_points = group['frame'].diff().fillna(1) < 0\n",
    "        if reset_points.any():\n",
    "            unique_sequences.append(group[reset_points])\n",
    "        else:\n",
    "            # If no reset points, consider the whole group as unique\n",
    "            unique_sequences.append(group.iloc[[0]])\n",
    "\n",
    "    # Concatenate unique sequences\n",
    "    df_unique = pd.concat(unique_sequences).reset_index(drop=True)\n",
    "    return df_unique[\"gesture\"]\n",
    "\n",
    "\n",
    "y_train_reshaped = reshape_y_labels(y_train)\n",
    "y_test_reshaped = reshape_y_labels(y_test)\n",
    "\n",
    "print(y_train_reshaped.shape, y_test_reshaped.shape)\n",
    "\n",
    "y_train_encoded, labels = pd.factorize(y_train_reshaped)\n",
    "y_test_encoded, _ = pd.factorize(y_test_reshaped)\n",
    "class_labels = labels\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=len(labels))\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=len(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [1721617027697291200, 1721617035044262600, 1721617044341822500, 1721617053827646200, 1721617062115648000, 1721617070322155500, 1721617078635838400, 1721617087687420100, 1721617094888227100, 1721617102118686200, 1721617109386097100, 1721617116502778300, 1721617123704757000, 1721617130878078000, 1721617512655027300, 1721617518762291100, 1721617524726805800, 1721617530743470200, 1721617536743659800, 1721617542854370800, 1721617548850003800, 1721617555008531900, 1721617561073103200, 1721617567194924300, 1721617573423100100, 1721617579495190400, 1721617585481057400, 1721617591616257300, 1721617812796674500, 1721617819149660900, 1721617825538925900, 1721617831786486200, 1721617838053639900, 1721617844766273800, 1721617851086522600, 1721617857159002000, 1721617863553882100, 1721617869780379700, 1721617875838491100, 1721617882367760300, 1721617889361363800, 1721618180137850100, 1721618186665596700, 1721618192782339000, 1721618199059574000, 1721618205946625700, 1721618212325947300, 1721618218885319400, 1721618225548809500, 1721618231799518800, 1721618238208589400, 1721618244661163100, 1721618251255223500, 1721618257541427200, 1721618263578593400, 1721618557401951500, 1721618563557380100, 1721618569654949200, 1721618575804731700, 1721618581839841500, 1721618587874741100, 1721618593831839800, 1721618599829908200, 1721618605818610300, 1721618611867519100, 1721618618493059000, 1721618624558067100, 1721618630536181500, 1721618636602022000, 3443377822327048600, 3443377829674020000, 3443377838971579900, 3443377848457403600, 3443377856745405400, 3443377864951912900, 3443377873265595800, 3443377882317177500, 3443377889517984500, 3443377896748443600, 3443377904015854500, 3443377911132535700, 3443377918334514400, 3443377925507835400, 3443378307284784700, 3443378313392048500, 3443378319356563200, 3443378325373227600, 3443378331373417200, 3443378337484128200, 3443378343479761200, 3443378349638289300, 3443378355702860600, 3443378361824681700, 3443378368052857500, 3443378374124947800, 3443378380110814800, 3443378386246014700, 3443378607426431900, 3443378613779418300, 3443378620168683300, 3443378626416243600, 3443378632683397300, 3443378639396031200, 3443378645716280000, 3443378651788759400, 3443378658183639500, 3443378664410137100, 3443378670468248500, 3443378676997517700, 3443378683991121200, 3443378974767607500, 3443378981295354100, 3443378987412096400, 3443378993689331400, 3443379000576383100, 3443379006955704700, 3443379013515076800, 3443379020178566900, 3443379026429276200, 3443379032838346800, 3443379039290920500, 3443379045884980900, 3443379052171184600, 3443379058208350800, 3443379352031708900, 3443379358187137500, 3443379364284706600, 3443379370434489100, 3443379376469598900, 3443379382504498500, 3443379388461597200, 3443379394459665600, 3443379400448367700, 3443379406497276500, 3443379413122816400, 3443379419187824500, 3443379425165938900, 3443379431231779400] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\pipeline.py\", line 1283, in _transform_one\n    res = transformer.transform(X, **params.transform)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\pipeline.py\", line 905, in transform\n    Xt = transform.transform(Xt, **routed_params[name].transform)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1023, in transform\n    X_int, X_mask = self._transform(\n  File \"c:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 213, in _transform\n    raise ValueError(msg)\nValueError: Found unknown categories [1721617027697291200, 1721617035044262600, 1721617044341822500, 1721617053827646200, 1721617062115648000, 1721617070322155500, 1721617078635838400, 1721617087687420100, 1721617094888227100, 1721617102118686200, 1721617109386097100, 1721617116502778300, 1721617123704757000, 1721617130878078000, 1721617512655027300, 1721617518762291100, 1721617524726805800, 1721617530743470200, 1721617536743659800, 1721617542854370800, 1721617548850003800, 1721617555008531900, 1721617561073103200, 1721617567194924300, 1721617573423100100, 1721617579495190400, 1721617585481057400, 1721617591616257300, 1721617812796674500, 1721617819149660900, 1721617825538925900, 1721617831786486200, 1721617838053639900, 1721617844766273800, 1721617851086522600, 1721617857159002000, 1721617863553882100, 1721617869780379700, 1721617875838491100, 1721617882367760300, 1721617889361363800, 1721618180137850100, 1721618186665596700, 1721618192782339000, 1721618199059574000, 1721618205946625700, 1721618212325947300, 1721618218885319400, 1721618225548809500, 1721618231799518800, 1721618238208589400, 1721618244661163100, 1721618251255223500, 1721618257541427200, 1721618263578593400, 1721618557401951500, 1721618563557380100, 1721618569654949200, 1721618575804731700, 1721618581839841500, 1721618587874741100, 1721618593831839800, 1721618599829908200, 1721618605818610300, 1721618611867519100, 1721618618493059000, 1721618624558067100, 1721618630536181500, 1721618636602022000, 3443377822327048600, 3443377829674020000, 3443377838971579900, 3443377848457403600, 3443377856745405400, 3443377864951912900, 3443377873265595800, 3443377882317177500, 3443377889517984500, 3443377896748443600, 3443377904015854500, 3443377911132535700, 3443377918334514400, 3443377925507835400, 3443378307284784700, 3443378313392048500, 3443378319356563200, 3443378325373227600, 3443378331373417200, 3443378337484128200, 3443378343479761200, 3443378349638289300, 3443378355702860600, 3443378361824681700, 3443378368052857500, 3443378374124947800, 3443378380110814800, 3443378386246014700, 3443378607426431900, 3443378613779418300, 3443378620168683300, 3443378626416243600, 3443378632683397300, 3443378639396031200, 3443378645716280000, 3443378651788759400, 3443378658183639500, 3443378664410137100, 3443378670468248500, 3443378676997517700, 3443378683991121200, 3443378974767607500, 3443378981295354100, 3443378987412096400, 3443378993689331400, 3443379000576383100, 3443379006955704700, 3443379013515076800, 3443379020178566900, 3443379026429276200, 3443379032838346800, 3443379039290920500, 3443379045884980900, 3443379052171184600, 3443379058208350800, 3443379352031708900, 3443379358187137500, 3443379364284706600, 3443379370434489100, 3443379376469598900, 3443379382504498500, 3443379388461597200, 3443379394459665600, 3443379400448367700, 3443379406497276500, 3443379413122816400, 3443379419187824500, 3443379425165938900, 3443379431231779400] in column 0 during transform\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m preprocess_pipeline(time_series_columns, numerical_columns, categorical_columns)\n\u001b[0;32m     50\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(X_train_fe)\n\u001b[1;32m---> 51\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_fe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1014\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[1;32m-> 1014\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:823\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[1;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[0;32m    811\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    812\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    813\u001b[0m             delayed(func)(\n\u001b[0;32m    814\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m             )\n\u001b[0;32m    821\u001b[0m         )\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories [1721617027697291200, 1721617035044262600, 1721617044341822500, 1721617053827646200, 1721617062115648000, 1721617070322155500, 1721617078635838400, 1721617087687420100, 1721617094888227100, 1721617102118686200, 1721617109386097100, 1721617116502778300, 1721617123704757000, 1721617130878078000, 1721617512655027300, 1721617518762291100, 1721617524726805800, 1721617530743470200, 1721617536743659800, 1721617542854370800, 1721617548850003800, 1721617555008531900, 1721617561073103200, 1721617567194924300, 1721617573423100100, 1721617579495190400, 1721617585481057400, 1721617591616257300, 1721617812796674500, 1721617819149660900, 1721617825538925900, 1721617831786486200, 1721617838053639900, 1721617844766273800, 1721617851086522600, 1721617857159002000, 1721617863553882100, 1721617869780379700, 1721617875838491100, 1721617882367760300, 1721617889361363800, 1721618180137850100, 1721618186665596700, 1721618192782339000, 1721618199059574000, 1721618205946625700, 1721618212325947300, 1721618218885319400, 1721618225548809500, 1721618231799518800, 1721618238208589400, 1721618244661163100, 1721618251255223500, 1721618257541427200, 1721618263578593400, 1721618557401951500, 1721618563557380100, 1721618569654949200, 1721618575804731700, 1721618581839841500, 1721618587874741100, 1721618593831839800, 1721618599829908200, 1721618605818610300, 1721618611867519100, 1721618618493059000, 1721618624558067100, 1721618630536181500, 1721618636602022000, 3443377822327048600, 3443377829674020000, 3443377838971579900, 3443377848457403600, 3443377856745405400, 3443377864951912900, 3443377873265595800, 3443377882317177500, 3443377889517984500, 3443377896748443600, 3443377904015854500, 3443377911132535700, 3443377918334514400, 3443377925507835400, 3443378307284784700, 3443378313392048500, 3443378319356563200, 3443378325373227600, 3443378331373417200, 3443378337484128200, 3443378343479761200, 3443378349638289300, 3443378355702860600, 3443378361824681700, 3443378368052857500, 3443378374124947800, 3443378380110814800, 3443378386246014700, 3443378607426431900, 3443378613779418300, 3443378620168683300, 3443378626416243600, 3443378632683397300, 3443378639396031200, 3443378645716280000, 3443378651788759400, 3443378658183639500, 3443378664410137100, 3443378670468248500, 3443378676997517700, 3443378683991121200, 3443378974767607500, 3443378981295354100, 3443378987412096400, 3443378993689331400, 3443379000576383100, 3443379006955704700, 3443379013515076800, 3443379020178566900, 3443379026429276200, 3443379032838346800, 3443379039290920500, 3443379045884980900, 3443379052171184600, 3443379058208350800, 3443379352031708900, 3443379358187137500, 3443379364284706600, 3443379370434489100, 3443379376469598900, 3443379382504498500, 3443379388461597200, 3443379394459665600, 3443379400448367700, 3443379406497276500, 3443379413122816400, 3443379419187824500, 3443379425165938900, 3443379431231779400] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
    "\n",
    "\n",
    "def preprocess_pipeline(timeseries_columns, numerical_columns, categorical_columns):\n",
    "    ts_numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)), # might want to change this out back to the interpolatioon methods\n",
    "        ('imputer2', SimpleImputer(strategy=\"mean\")),\n",
    "        ('scaler', MinMaxScaler())\n",
    "        # ('smoother', FunctionTransformer(lambda x: x.rolling(window=3, min_periods=1).mean())),\n",
    "        # ('differencing', FunctionTransformer(lambda x: x.diff().fillna(0))),\n",
    "        # ('lag_features', FunctionTransformer(lambda x: pd.concat([x.shift(i) for i in range(1, 4)], axis=1).fillna(0))),\n",
    "        # ('rolling_stats', FunctionTransformer(lambda x: pd.concat([x.rolling(window=3).mean(), x.rolling(window=3).std()], axis=1).fillna(0)))\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "        # ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # Polynomial features\n",
    "        # ('power', PowerTransformer(method='yeo-johnson')),   \n",
    "        (\"normalize\", StandardScaler()),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('pca', PCA(n_components=10))\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")), # technically this is wrong\n",
    "        (\"ohe\", OneHotEncoder(sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ts_num', ts_numerical_transformer, timeseries_columns),\n",
    "            ('num', numerical_transformer, numerical_columns),\n",
    "            ('cat', categorical_transformer, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        sparse_threshold=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    " \n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = preprocess_pipeline(time_series_columns, numerical_columns, categorical_columns)\n",
    "X_train_transformed = preprocessor.fit_transform(X_train_fe)\n",
    "X_test_transformed = preprocessor.transform(X_test_fe) # need to redo that gesture_index thing\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
