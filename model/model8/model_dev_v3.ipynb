{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data formatting might be a problem\n",
    "import csv\n",
    "import math\n",
    "import os  \n",
    "import time \n",
    "\n",
    "from keras._tf_keras.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras._tf_keras.keras.metrics import MeanAbsoluteError, Accuracy, Precision, Recall, MeanSquaredError\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "from keras._tf_keras.keras.optimizers import Adam , RMSprop, Nadam\n",
    "from keras._tf_keras.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Masking, InputLayer, Conv1D, MaxPooling1D, Flatten, TimeDistributed, LayerNormalization, Activation\n",
    "from keras._tf_keras.keras.regularizers import L1L2, L1, L2\n",
    "from keras._tf_keras.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder \n",
    "\n",
    "from FeatureEngineering import FeatureEngineering as fe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_from_data(input_path: str):\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(input_path):\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        gesture = file_name.split(\"_\")[0]\n",
    "        gesture_index = int(file_name.split(\"_\")[1].split(\".\")[0])\n",
    "        \n",
    "        # add two new columns to df \n",
    "        df[\"gesture\"] = gesture\n",
    "        df[\"gesture_index\"] = gesture_index\n",
    "\n",
    "        dataframes.append(df)\n",
    "        df.sort_values(by=\"frame\", inplace=True)\n",
    "    \n",
    "    return pd.concat(dataframes, ignore_index=True) if len(dataframes) > 0 else ValueError(\"Dataframe is empty\")\n",
    "\n",
    "def create_dict_from_df(df: pd.DataFrame):\n",
    "    diction = {x: [] for x in np.unique(df[\"gesture\"].values.tolist())}\n",
    "    for gesture_index, gesture_data in df.groupby(\"gesture_index\"):\n",
    "        gesture = np.unique(gesture_data[\"gesture\"].values.tolist())[0]\n",
    "        tmp = diction[gesture] + [gesture_index]\n",
    "        diction.update({gesture:tmp})\n",
    "    return diction\n",
    "\n",
    "def transform_to_sequences(df: pd.DataFrame, sequence_length, target: str, additional_targets: list = None):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    grouped = df.groupby('gesture_index')\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('frame').reset_index(drop=True)\n",
    "        for i in range(0, len(group), sequence_length):\n",
    "            sequence = group.iloc[i:i+sequence_length].drop(columns=['gesture_index']).values # gesture_index is dropped because it's more like metadata, no value as a features\n",
    "            if len(sequence) == sequence_length:\n",
    "                sequences.append(sequence)\n",
    "                labels.append(group.iloc[0][[target] + additional_targets]) if additional_targets else labels.append(group.iloc[0][target])\n",
    "    return np.array(sequences), np.array(labels) \n",
    "\n",
    "def split_dataset(df: pd.DataFrame, target_label: str, sequence_length=30, additional_targets: list=None, train_ratio=0.7, val_ratio=0.15 ,test_ratio=0.15):\n",
    "    # probably make this train, val, test\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"ratios must sum to 1.\"\n",
    "\n",
    "    gesture_index_dict = create_dict_from_df(df)\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for _, indices in gesture_index_dict.items():\n",
    "        n_total = len(indices)\n",
    "        n_train = math.ceil(n_total * train_ratio)\n",
    "        n_val = math.ceil(n_total * val_ratio)\n",
    "        \n",
    "        train_indices.extend(indices[:n_train])\n",
    "        val_indices.extend(indices[n_train:n_train + n_val])\n",
    "        test_indices.extend(indices[n_train + n_val:])\n",
    "\n",
    "    grouped_data = df.groupby(\"gesture_index\")\n",
    "\n",
    "    train_frames, val_frames, test_frames = [], [], []\n",
    "    for idx in train_indices:\n",
    "        train_frames.append(grouped_data.get_group(idx))\n",
    "        \n",
    "    for idx in val_indices:\n",
    "        val_frames.append(grouped_data.get_group(idx))\n",
    "        \n",
    "    for idx in test_indices:\n",
    "        test_frames.append(grouped_data.get_group(idx))\n",
    "\n",
    "    # Concatenate the dataframes to create the final train and test sets\n",
    "    train_set = pd.concat(train_frames).reset_index(drop=True)\n",
    "    val_set = pd.concat(val_frames).reset_index(drop=True)\n",
    "    test_set = pd.concat(test_frames).reset_index(drop=True)\n",
    "    \n",
    "    # X_train_sequences, y_train_sequences = transform_to_sequences(train_set,  sequence_length, target_label, additional_targets)\n",
    "    # X_val_sequences, y_val_sequences = transform_to_sequences(val_set,  sequence_length, target_label, additional_targets)\n",
    "    # X_test_sequences, y_test_sequences = transform_to_sequences(test_set,  sequence_length, target_label, additional_targets)\n",
    "\n",
    "    # Separate X and y\n",
    "    # X_train = X_train_sequences\n",
    "    # y_train = y_train_sequences\n",
    "    # X_val = X_val_sequences\n",
    "    # y_val = y_val_sequences\n",
    "    # X_test = X_test_sequences\n",
    "    # y_test = y_test_sequences\n",
    "\n",
    "    X_train = train_set.drop(columns=[target_label])\n",
    "    y_train = train_set[[target_label] + additional_targets] if additional_targets else train_set[[target_label]]\n",
    "    X_val = val_set.drop(columns=[target_label])\n",
    "    y_val = val_set[[target_label] + additional_targets] if additional_targets else train_set[[target_label]]\n",
    "    X_test = test_set.drop(columns=[target_label])\n",
    "    y_test = test_set[[target_label] + additional_targets] if additional_targets else test_set[[target_label]]\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def save_sequences_to_csv(sequences, labels, filename, df):\n",
    "    with open(filename, mode=\"w\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = []      \n",
    "        for i in range(30):\n",
    "            for col in df.columns[:-2]:  # Exclude 'gesture_index' and 'gesture' columns\n",
    "                header.append(f\"{col}_frame_{i}\")\n",
    "        header.append(\"gesture\")\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Write data\n",
    "        for sequence, label in zip(sequences, labels):\n",
    "            flattened_sequence = sequence.flatten()\n",
    "            writer.writerow(np.append(flattened_sequence, label))\n",
    "\n",
    "def calculate_hand_motion_feature(df: pd.DataFrame, landmark_cols: list):\n",
    "    df_copy = df.copy()\n",
    "    s = time.process_time()\n",
    "    df_temporal = fe.calculate_temporal_features(df_copy, landmark_cols)\n",
    "    print(time.process_time()-s)\n",
    "    # Ensure there are no duplicate columns\n",
    "    df_combined = df_copy.loc[:,~df_copy.columns.duplicated()]\n",
    "    return df_combined\n",
    "\n",
    "def preprocess_pipeline(timeseries_columns, numerical_columns, categorical_columns):\n",
    "    ts_numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)), # might want to change this out back to the interpolatioon methods\n",
    "        ('imputer2', SimpleImputer(strategy=\"mean\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "        (\"normalize\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")), # technically this is wrong\n",
    "        (\"ohe\", OneHotEncoder(sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ts_num', ts_numerical_transformer, timeseries_columns),\n",
    "            ('num', numerical_transformer, numerical_columns),\n",
    "            # ('cat', categorical_transformer, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        sparse_threshold=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    " \n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "    \n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7320, 1667) (7320, 1) (1650, 1667) (7320, 1) (1470, 1667) (1470, 1)\n",
      "53.859375\n",
      "10.5625\n",
      "9.53125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = r'C:\\Users\\Gen3r\\Documents\\capstone\\ml_model\\data\\data_3'\n",
    "dataframe = create_dataframe_from_data(input_path)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_dataset(dataframe, target_label='gesture')\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "landmark_columns = [f\"{col}\" for col in dataframe.columns if col.startswith((\"hx\", \"hy\", \"hz\", \"px\", \"py\", \"pz\", \"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "categorical_columns = [\"gesture_index\"]\n",
    "numerical_columns = [\"frame\", \"frame_rate\", \"frame_width\", \"frame_height\"] + [f\"{col}\" for col in dataframe.columns if col.startswith(\"pose_visibility\")]\n",
    "derived_features =  [f\"{feat}_{col}\" for feat in [\"velocity\", \"acceleration\", \"jerk\"] for col in landmark_columns if col.startswith((\"lx\", \"ly\", \"lz\", \"rx\", \"ry\", \"rz\"))]\n",
    "time_series_columns = landmark_columns + derived_features     \n",
    "res = [item for item in landmark_columns if item.startswith((\"r\", \"l\"))]\n",
    "\n",
    "\n",
    "X_train_fe = calculate_hand_motion_feature(X_train, res)\n",
    "X_val_fe = calculate_hand_motion_feature(X_val, res)\n",
    "X_test_fe = calculate_hand_motion_feature(X_test, res)\n",
    "\n",
    "preprocessor = preprocess_pipeline(time_series_columns, numerical_columns, categorical_columns)\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train_fe)\n",
    "X_val_transformed = preprocessor.transform(X_val_fe)\n",
    "X_test_transformed = preprocessor.transform(X_test_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7320, 2045)\n",
      "(1650, 2045)\n",
      "(1470, 2045)\n"
     ]
    }
   ],
   "source": [
    "def transform_to_sequences(df: pd.DataFrame, sequence_length, target: str, additional_targets: list = None):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    grouped = df.groupby('gesture_index')\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('frame').reset_index(drop=True)\n",
    "        for i in range(0, len(group) - sequence_length + 1, sequence_length):\n",
    "            sequence = group.iloc[i:i+sequence_length].drop(columns=['gesture_index']).values\n",
    "            if len(sequence) == sequence_length:\n",
    "                sequences.append(sequence)\n",
    "                labels.append(group.iloc[0][target]) if additional_targets is None else labels.append(group.iloc[0][[target] + additional_targets])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Assuming the target column name is 'gesture' and no additional targets\n",
    "def create_sequences_with_labels(X_transformed, y, sequence_length):\n",
    "    # Combine features and labels into a single DataFrame\n",
    "    combined_df = pd.concat([pd.DataFrame(X_transformed), y.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Convert the DataFrame to sequences\n",
    "    X_sequences, y_sequences = transform_to_sequences(combined_df, sequence_length, target='gesture')\n",
    "    \n",
    "    return X_sequences, y_sequences\n",
    "\n",
    "\n",
    "print(X_train_transformed.shape)\n",
    "print(X_val_transformed.shape)\n",
    "print(X_test_transformed.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
